{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaitlynchen1/PredictingHousingPrices/blob/main/housingprices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ted8080 House Prices & Images** Kaggle dataset"
      ],
      "metadata": {
        "id": "hxogi7mh-PSJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i8xZLGRSoc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb30f8b-5e7d-4a19-81c4-0e6c0f6662f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in dataset: ['socal2.csv', 'socal2']\n",
            "   image_id                 street             citi  n_citi  bed  bath  sqft  \\\n",
            "0         0  1317 Van Buren Avenue  Salton City, CA     317    3   2.0  1560   \n",
            "1         1         124 C Street W      Brawley, CA      48    3   2.0   713   \n",
            "2         2        2304 Clark Road     Imperial, CA     152    3   1.0   800   \n",
            "3         3     755 Brawley Avenue      Brawley, CA      48    3   1.0  1082   \n",
            "4         4  2207 R Carrillo Court     Calexico, CA      55    4   3.0  2547   \n",
            "\n",
            "    price  \n",
            "0  201900  \n",
            "1  228500  \n",
            "2  273950  \n",
            "3  350000  \n",
            "4  385100  \n",
            "           image_id        n_citi           bed          bath          sqft  \\\n",
            "count  15474.000000  15474.000000  15474.000000  15474.000000  15474.000000   \n",
            "mean    7736.500000    216.597518      3.506398      2.453251   2173.913209   \n",
            "std     4467.103368    112.372985      1.034838      0.958742   1025.339617   \n",
            "min        0.000000      0.000000      1.000000      0.000000    280.000000   \n",
            "25%     3868.250000    119.000000      3.000000      2.000000   1426.000000   \n",
            "50%     7736.500000    222.500000      3.000000      2.100000   1951.000000   \n",
            "75%    11604.750000    315.000000      4.000000      3.000000   2737.750000   \n",
            "max    15473.000000    414.000000     12.000000     36.000000  17667.000000   \n",
            "\n",
            "              price  \n",
            "count  1.547400e+04  \n",
            "mean   7.031209e+05  \n",
            "std    3.769762e+05  \n",
            "min    1.950000e+05  \n",
            "25%    4.450000e+05  \n",
            "50%    6.390000e+05  \n",
            "75%    8.349750e+05  \n",
            "max    2.000000e+06  \n",
            "image_id      int64\n",
            "street       object\n",
            "citi         object\n",
            "n_citi        int64\n",
            "bed           int64\n",
            "bath        float64\n",
            "sqft          int64\n",
            "price         int64\n",
            "dtype: object\n",
            "(15474, 8)\n",
            "Shape X (input matrix): (15474, 12821)\n",
            "Shape Y (output matrix): (15474, 1)\n",
            "Shape X_train: (12379, 12821)\n",
            "Shape X_test: (3095, 12821)\n",
            "Shape Y_train: (12379, 1)\n",
            "Shape Y_test: (3095, 1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Download dataset using Kaggle API\n",
        "os.system('kaggle datasets download -d ted8080/house-prices-and-images-socal')\n",
        "\n",
        "# Unzip file\n",
        "with zipfile.ZipFile('house-prices-and-images-socal.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('house-prices-and-images-socal')\n",
        "\n",
        "# List files, find the CSV file\n",
        "path = 'house-prices-and-images-socal'\n",
        "files = os.listdir(path)\n",
        "print(\"Files in dataset:\", files)\n",
        "\n",
        "csv_files = [f for f in files if f.endswith('.csv')]\n",
        "if csv_files:\n",
        "    dataset_path = os.path.join(path, csv_files[0])  # Choose the first CSV file\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    # first few rows\n",
        "    print(df.head())\n",
        "\n",
        "    # Summary statistics\n",
        "    print(df.describe())\n",
        "\n",
        "    # Data types of columns\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Number of rows and columns\n",
        "    print(df.shape)\n",
        "\n",
        "    # Preprocessing\n",
        "    target_column = 'price'\n",
        "    if target_column in df.columns:\n",
        "        Y = df[target_column].values  # Output matrix\n",
        "        X = df.drop(columns=[target_column])  # Input matrix\n",
        "\n",
        "        # Handling missing values and encoding categorical variables, and scale numerical features\n",
        "        numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "        categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "        # Preprocessing pipeline\n",
        "        numerical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ])\n",
        "\n",
        "        # Apply preprocessing\n",
        "        X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "        # Convert Y to a 2D array\n",
        "        Y = Y.reshape(-1, 1)\n",
        "\n",
        "        # Display shapes of X and Y\n",
        "        print(\"Shape X (input matrix):\", X_processed.shape)\n",
        "        print(\"Shape Y (output matrix):\", Y.shape)\n",
        "\n",
        "        # Split into train and test sets\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(X_processed, Y, test_size=0.2, random_state=42)\n",
        "        print(\"Shape X_train:\", X_train.shape)\n",
        "        print(\"Shape X_test:\", X_test.shape)\n",
        "        print(\"Shape Y_train:\", Y_train.shape)\n",
        "        print(\"Shape Y_test:\", Y_test.shape)\n",
        "else:\n",
        "    print(\"Issue finding Price column\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exterior housing images and pricing dataset containing 8 variables & 15000+ rows in SoCal\n",
        "! kaggle datasets download robinreni/house-rooms-image-dataset\n",
        "\n",
        "# Around 3000 collective images of Bathroom, Bedroom, Living Room, Dining, & Kitchen spaces (does not contain price variable)\n",
        "! kaggle datasets download mikhailma/house-rooms-streets-image-dataset\n",
        "\n",
        "!wget https://github.com/emanhamed/Houses-dataset/tree/master/Houses%20Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBfYC6Wsef0E",
        "outputId": "ac678d2e-1593-4128-966a-0b323bbf4c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/robinreni/house-rooms-image-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading house-rooms-image-dataset.zip to /content\n",
            " 91% 106M/116M [00:00<00:00, 200MB/s] \n",
            "100% 116M/116M [00:00<00:00, 202MB/s]\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/mikhailma/house-rooms-streets-image-dataset\n",
            "License(s): CC0-1.0\n",
            "house-rooms-streets-image-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "--2025-03-20 14:38:29--  https://github.com/emanhamed/Houses-dataset/tree/master/Houses%20Dataset\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Houses Dataset’\n",
            "\n",
            "Houses Dataset          [ <=>                ] 508.19K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-03-20 14:38:30 (7.77 MB/s) - ‘Houses Dataset’ saved [520390]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Robin Reni Rooms Images** Kaggle dataset (No price)"
      ],
      "metadata": {
        "id": "WoRImSMe91HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "\n",
        "# Download Robin Reni dataset\n",
        "path = kagglehub.dataset_download(\"robinreni/house-rooms-image-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Extract zip file\n",
        "dataset_folder = \"/content/datasets/House_Room_Dataset\"\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "# Ensure the zip file is extracted\n",
        "zip_path = os.path.join(dataset_folder, \"house-rooms-image-dataset.zip\")\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Extracting {zip_path} to {dataset_folder}\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_folder)\n",
        "    print(f\"Dataset extracted to {dataset_folder}\")\n",
        "else:\n",
        "    print(f\"Zip file not found at {zip_path}\")\n",
        "\n",
        "# Verify extraction\n",
        "print(\"Contents of dataset folder:\", os.listdir(dataset_folder))\n",
        "\n",
        "# Navigate into the correct subfolder\n",
        "dataset_subfolder = os.path.join(dataset_folder, \"House_Room_Dataset\")\n",
        "print(\"Contents of dataset subfolder:\", os.listdir(dataset_subfolder))\n",
        "\n",
        "# Image resize\n",
        "IMAGE_SIZE = (128, 128)\n",
        "\n",
        "def load_images_from_folder(folder, num_samples=None):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(folder))  # Get list of subfolders\n",
        "    print(\"Class names:\", class_names)  # Debug: Print class names\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_folder = os.path.join(folder, class_name)\n",
        "        if not os.path.isdir(class_folder):\n",
        "            print(f\"Skipping non-folder: {class_folder}\")  # Debug: Skip non-folders\n",
        "            continue\n",
        "\n",
        "        print(f\"Loading images from class: {class_name}\")  # Debug: Print class name\n",
        "        for filename in tqdm(os.listdir(class_folder)):\n",
        "            img_path = os.path.join(class_folder, filename)\n",
        "            # print(f\"Processing file: {img_path}\")  # Debug: Print file path\n",
        "\n",
        "            # Check for valid image extensions\n",
        "            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                print(f\"Skip non-image: {img_path}\")  # Debug: Skip non-images\n",
        "                continue\n",
        "\n",
        "            # Load and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Failed to load image: {img_path}\")  # Debug: Skip corrupted images\n",
        "                continue\n",
        "\n",
        "            img = cv2.resize(img, IMAGE_SIZE)\n",
        "            img = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "            images.append(img)\n",
        "            labels.append(class_names.index(class_name))  # Assign label based on folder name\n",
        "\n",
        "            # Limit the number of samples if specified\n",
        "            if num_samples and len(images) >= num_samples:\n",
        "                break\n",
        "\n",
        "        # Break outer loop if num_samples is reached\n",
        "        if num_samples and len(images) >= num_samples:\n",
        "            break\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load images from dataset subfolder\n",
        "X_images, Y = load_images_from_folder(dataset_subfolder, num_samples=5250)\n",
        "print(f\"Loaded {X_images.shape[0]} images from the dataset.\")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train_images, X_test_images, Y_train, Y_test = train_test_split(\n",
        "    X_images, Y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Save the preprocessed data\n",
        "os.makedirs(\"preprocessed_data\", exist_ok=True)\n",
        "np.save(\"preprocessed_data/X_train_images.npy\", X_train_images)\n",
        "np.save(\"preprocessed_data/X_test_images.npy\", X_test_images)\n",
        "np.save(\"preprocessed_data/Y_train.npy\", Y_train)\n",
        "np.save(\"preprocessed_data/Y_test.npy\", Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JEY_N2d06Z-",
        "outputId": "f1549d25-6f9c-4ffd-82ca-7cce54e0b4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/robinreni/house-rooms-image-dataset/versions/1\n",
            "Extracting /content/datasets/House_Room_Dataset/house-rooms-image-dataset.zip to /content/datasets/House_Room_Dataset\n",
            "Dataset extracted to /content/datasets/House_Room_Dataset\n",
            "Contents of dataset folder: ['house-rooms-image-dataset.zip', 'House_Room_Dataset']\n",
            "Contents of dataset subfolder: ['Kitchen', 'Livingroom', 'Dinning', 'Bedroom', 'Bathroom']\n",
            "Class names: ['Bathroom', 'Bedroom', 'Dinning', 'Kitchen', 'Livingroom']\n",
            "Loading images from class: Bathroom\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 606/606 [00:00<00:00, 1007.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from class: Bedroom\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1248/1248 [00:01<00:00, 939.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from class: Dinning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1158/1158 [00:01<00:00, 874.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from class: Kitchen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 965/965 [00:01<00:00, 946.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from class: Livingroom\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1273/1273 [00:01<00:00, 920.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5250 images from the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mikhail Ma House Rooms & Streets** Kaggle dataset (No Price)"
      ],
      "metadata": {
        "id": "6WohoDqm9erW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "\n",
        "# Download the dataset\n",
        "dataset_slug = \"mikhailma/house-rooms-streets-image-dataset\"\n",
        "path = kagglehub.dataset_download(dataset_slug)\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Extract zip file\n",
        "dataset_folder = \"/content/datasets/House_Rooms_Streets_Dataset\"\n",
        "if not os.path.exists(dataset_folder):\n",
        "    os.makedirs(dataset_folder)\n",
        "\n",
        "# Check if zip file has been properly extracted\n",
        "zip_path = os.path.join(dataset_folder, \"house-rooms-streets-image-dataset.zip\")\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Extracting {zip_path} to {dataset_folder}\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_folder)\n",
        "    print(f\"Dataset extracted to {dataset_folder}\")\n",
        "else:\n",
        "    print(f\"Zip file not found at {zip_path}\")\n",
        "\n",
        "# Confirm extraction\n",
        "print(\"Contents of dataset folder:\", os.listdir(dataset_folder))\n",
        "\n",
        "# Navigate to correct subfolder\n",
        "dataset_subfolder = os.path.join(dataset_folder, \"kaggle_room_street_data\")\n",
        "print(\"Contents of dataset subfolder:\", os.listdir(dataset_subfolder))\n",
        "\n",
        "# Image resize\n",
        "IMAGE_SIZE = (128, 128)\n",
        "\n",
        "def load_images_from_folder(folder, street_data_limit=1000):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(folder))  # Get list of subfolders\n",
        "    print(\"Class names:\", class_names)  # Print class names\n",
        "\n",
        "    street_data_count = 0  # Count for street_data images\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_folder = os.path.join(folder, class_name)\n",
        "        if not os.path.isdir(class_folder):\n",
        "            print(f\"Skipping non-folder: {class_folder}\")  # Skip non-folders\n",
        "            continue\n",
        "\n",
        "        print(f\"Loading images from class: {class_name}\")  # Print class name\n",
        "        for filename in tqdm(os.listdir(class_folder)):\n",
        "            # Set a limit to save RAM usage\n",
        "            if class_name == \"street_data\" and street_data_count >= street_data_limit:\n",
        "                print(f\"Reached street_data limit of {street_data_limit} images.\")\n",
        "                break\n",
        "\n",
        "            img_path = os.path.join(class_folder, filename)\n",
        "\n",
        "            # Check for valid image extensions\n",
        "            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                print(f\"Skip non-image: {img_path}\")  # Skip non-images\n",
        "                continue\n",
        "\n",
        "            # Load and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Failed to load image: {img_path}\") # Skip potentially corrupted images\n",
        "                continue\n",
        "\n",
        "            img = cv2.resize(img, IMAGE_SIZE)\n",
        "            img = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "            images.append(img)\n",
        "            labels.append(class_names.index(class_name))  # Assign label based on folder name\n",
        "\n",
        "            # Increment street_data counter if applicable\n",
        "            if class_name == \"street_data\":\n",
        "                street_data_count += 1\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load images, set limit to limit RAM usage\n",
        "X_images, Y = load_images_from_folder(dataset_subfolder, street_data_limit=5000)\n",
        "print(f\"Loaded {X_images.shape[0]} images from the dataset.\")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train_images, X_test_images, Y_train, Y_test = train_test_split(\n",
        "    X_images, Y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Save the preprocessed data\n",
        "os.makedirs(\"preprocessed_data\", exist_ok=True)\n",
        "np.save(\"preprocessed_data/X_train_images.npy\", X_train_images)\n",
        "np.save(\"preprocessed_data/X_test_images.npy\", X_test_images)\n",
        "np.save(\"preprocessed_data/Y_train.npy\", Y_train)\n",
        "np.save(\"preprocessed_data/Y_test.npy\", Y_test)"
      ],
      "metadata": {
        "id": "K77K683xBohU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588a2282-47ff-4dea-8228-63c49ba04535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/mikhailma/house-rooms-streets-image-dataset/versions/1\n",
            "Extracting /content/datasets/House_Rooms_Streets_Dataset/house-rooms-streets-image-dataset.zip to /content/datasets/House_Rooms_Streets_Dataset\n",
            "Dataset extracted to /content/datasets/House_Rooms_Streets_Dataset\n",
            "Contents of dataset folder: ['kaggle_room_street_data', 'house-rooms-streets-image-dataset.zip']\n",
            "Contents of dataset subfolder: ['street_data', 'house_data']\n",
            "Class names: ['house_data', 'street_data']\n",
            "Loading images from class: house_data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5249/5249 [00:04<00:00, 1120.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images from class: street_data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 5000/19658 [00:03<00:10, 1351.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reached street_data limit of 5000 images.\n",
            "Loaded 10249 images from the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GitHub** dataset (No price)\n",
        "\n",
        "Previously processed all images, but changes to the directories leaves us with a current issue. This should be fixable for later, however."
      ],
      "metadata": {
        "id": "Gz1SMqm29WIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls -l /content/datasets\n",
        "# file /content/datasets/Houses_Dataset\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Path to the folder containing images\n",
        "drive_path = \"/content/datasets/Houses_Dataset\"\n",
        "\n",
        "# Verify the path\n",
        "if not os.path.exists(drive_path):\n",
        "    raise FileNotFoundError(f\"The path {drive_path} does not exist.\")\n",
        "if not os.path.isdir(drive_path):\n",
        "    raise NotADirectoryError(f\"The path {drive_path} is not a directory.\")\n",
        "\n",
        "# Resize images\n",
        "IMAGE_SIZE = (128, 128)\n",
        "\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    filenames = []\n",
        "\n",
        "    # Iterate through all files in the folder\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "\n",
        "        # Verify image filetype\n",
        "        if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "            # Load image\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                print(f\"Warning: Unable to load image {filename}. Skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Resize and normalize image\n",
        "            img = cv2.resize(img, IMAGE_SIZE)\n",
        "            img = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
        "            images.append(img)\n",
        "            filenames.append(filename)\n",
        "        else:\n",
        "            print(f\"Skipping non-image file: {filename}\")\n",
        "\n",
        "    return np.array(images), filenames\n",
        "\n",
        "# Load all images\n",
        "X_images, image_filenames = load_images_from_folder(drive_path)\n",
        "\n",
        "# Save data\n",
        "output_path = os.path.join(drive_path, \"X_images.npy\")\n",
        "np.save(output_path, X_images)\n",
        "\n",
        "# Print confirmation\n",
        "print(f\"Images processed: {len(X_images)}\")\n",
        "print(f\"Processed images saved to: {output_path}\")\n",
        "\n",
        "!ls -l /content/datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "o6AMj5APwdEf",
        "outputId": "57c303b3-f072-4372-a2d4-1f80a1fb3459",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "The path /content/datasets/Houses_Dataset does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9d114a4d4e5e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Verify the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The path {drive_path} does not exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotADirectoryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The path {drive_path} is not a directory.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: The path /content/datasets/Houses_Dataset does not exist."
          ]
        }
      ]
    }
  ]
}