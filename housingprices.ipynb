{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ted8080 House Prices & Images** Kaggle dataset\n",
        "\n",
        "- Follow the link to the dataset: https://www.kaggle.com/datasets/ted8080/house-prices-and-images-socal\n",
        "\n",
        "- Set up Kaggle API keys\n",
        "\n",
        "- Click 'download' and download via kagglehub"
      ],
      "metadata": {
        "id": "hxogi7mh-PSJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8i8xZLGRSoc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811688b7-24e3-4cbd-db8a-1ed481fdb06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading:\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ted8080/house-prices-and-images-socal?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 369M/369M [00:07<00:00, 52.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/ted8080/house-prices-and-images-socal/versions/1\n",
            "Found CSV file at: /root/.cache/kagglehub/datasets/ted8080/house-prices-and-images-socal/versions/1/socal2.csv\n",
            "\n",
            "First 5 rows:\n",
            "   image_id                 street             citi  n_citi  bed  bath  sqft  \\\n",
            "0         0  1317 Van Buren Avenue  Salton City, CA     317    3   2.0  1560   \n",
            "1         1         124 C Street W      Brawley, CA      48    3   2.0   713   \n",
            "2         2        2304 Clark Road     Imperial, CA     152    3   1.0   800   \n",
            "3         3     755 Brawley Avenue      Brawley, CA      48    3   1.0  1082   \n",
            "4         4  2207 R Carrillo Court     Calexico, CA      55    4   3.0  2547   \n",
            "\n",
            "    price  \n",
            "0  201900  \n",
            "1  228500  \n",
            "2  273950  \n",
            "3  350000  \n",
            "4  385100  \n",
            "\n",
            "Dataset shape: (15474, 8)\n",
            "\n",
            "Column types:\n",
            "image_id      int64\n",
            "street       object\n",
            "citi         object\n",
            "n_citi        int64\n",
            "bed           int64\n",
            "bath        float64\n",
            "sqft          int64\n",
            "price         int64\n",
            "dtype: object\n",
            "\n",
            "Missing values per column:\n",
            "image_id    0\n",
            "street      0\n",
            "citi        0\n",
            "n_citi      0\n",
            "bed         0\n",
            "bath        0\n",
            "sqft        0\n",
            "price       0\n",
            "dtype: int64\n",
            "\n",
            "Numeric features (5): ['image_id', 'n_citi', 'bed', 'bath', 'sqft']\n",
            "Categorical features (2): ['street', 'citi']\n",
            "\n",
            "Preprocessing data...\n",
            "\n",
            "Processed features shape: (15474, 12821)\n",
            "Target shape: (15474, 1)\n",
            "\n",
            "Training set: (12379, 12821) (12379, 1)\n",
            "Test set: (3095, 12821) (3095, 1)\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import os\n",
        "\n",
        "try:\n",
        "    # Download dataset using kagglehub\n",
        "    print(\"Downloading:\")\n",
        "    download_path = kagglehub.dataset_download(\"ted8080/house-prices-and-images-socal\")\n",
        "    print(f\"Dataset downloaded to: {download_path}\")\n",
        "\n",
        "    # Find the CSV file in directory\n",
        "    csv_file = None\n",
        "    for root, dirs, files in os.walk(download_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv'):\n",
        "                csv_file = os.path.join(root, file)\n",
        "                break\n",
        "        if csv_file:\n",
        "            break\n",
        "\n",
        "    if not csv_file:\n",
        "        raise FileNotFoundError(\"No CSV file found\")\n",
        "\n",
        "    print(f\"Found CSV file at: {csv_file}\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Data exploration\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nDataset shape:\", df.shape)\n",
        "    print(\"\\nColumn types:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nMissing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # Preprocessing\n",
        "    target = 'price'\n",
        "    if target not in df.columns:\n",
        "        raise ValueError(f\"'{target}' column not found. Available columns: {list(df.columns)}\")\n",
        "\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target].values.reshape(-1, 1)\n",
        "\n",
        "    # Identify feature types\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "    print(f\"\\nNumeric features ({len(numeric_features)}):\", list(numeric_features))\n",
        "    print(f\"Categorical features ({len(categorical_features)}):\", list(categorical_features))\n",
        "\n",
        "    # Create preprocessing pipelines\n",
        "    numeric_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "    # Process data\n",
        "    print(\"\\nPreprocessing data...\")\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "    print(\"\\nProcessed features shape:\", X_processed.shape)\n",
        "    print(\"Target shape:\", y.shape)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_processed, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining set:\", X_train.shape, y_train.shape)\n",
        "    print(\"Test set:\", X_test.shape, y_test.shape)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError: {str(e)}\")\n",
        "    print(\"Ensure you have a proper kagglehub setup\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exterior housing images and pricing dataset containing 8 variables & 15000+ rows in SoCal\n",
        "! kaggle datasets download robinreni/house-rooms-image-dataset\n",
        "\n",
        "# Around 3000 collective images of Bathroom, Bedroom, Living Room, Dining, & Kitchen spaces (does not contain price variable)\n",
        "! kaggle datasets download mikhailma/house-rooms-streets-image-dataset\n",
        "\n",
        "!wget https://github.com/emanhamed/Houses-dataset/tree/master/Houses%20Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBfYC6Wsef0E",
        "outputId": "6b52d315-5ae1-4c4e-df67-6e00ccd20ce3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1734, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1734, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "--2025-04-25 12:26:33--  https://github.com/emanhamed/Houses-dataset/tree/master/Houses%20Dataset\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Houses Dataset’\n",
            "\n",
            "Houses Dataset          [ <=>                ] 511.87K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-04-25 12:26:34 (8.16 MB/s) - ‘Houses Dataset’ saved [524152]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Robin Reni Rooms Images** Kaggle dataset (No price)\n",
        "\n",
        "- Open the link and follow the steps from the previous code block: https://www.kaggle.com/datasets/robinreni/house-rooms-image-dataset"
      ],
      "metadata": {
        "id": "WoRImSMe91HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "\n",
        "# Constants\n",
        "IMAGE_SIZE = (128, 128)\n",
        "DATASET_NAME = \"robinreni/house-rooms-image-dataset\"\n",
        "PREPROCESSED_FOLDER = \"preprocessed_data\"\n",
        "NUM_SAMPLES = 5250  # Set to None\n",
        "\n",
        "def setup_directories():\n",
        "    \"\"\"Create necessary directories if they don't exist\"\"\"\n",
        "    os.makedirs(PREPROCESSED_FOLDER, exist_ok=True)\n",
        "\n",
        "def load_images_from_folder(folder, num_samples=None):\n",
        "    \"\"\"Load and preprocess images from folder structure\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted([d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d))])\n",
        "\n",
        "    if not class_names:\n",
        "        raise ValueError(f\"No class folders found in {folder}\")\n",
        "\n",
        "    print(\"Found classes:\", class_names)\n",
        "\n",
        "    for class_idx, class_name in enumerate(class_names):\n",
        "        class_folder = os.path.join(folder, class_name)\n",
        "        print(f\"\\nLoading images from: {class_folder}\")\n",
        "\n",
        "        image_files = [f for f in os.listdir(class_folder)\n",
        "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        if not image_files:\n",
        "            print(f\"Warning: No images found in {class_folder}\")\n",
        "            continue\n",
        "\n",
        "        for filename in tqdm(image_files[:num_samples] if num_samples else tqdm(image_files)):\n",
        "            img_path = os.path.join(class_folder, filename)\n",
        "\n",
        "            try:\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is None:\n",
        "                    print(f\"Warning: Failed to read {img_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Preprocess image\n",
        "                img = cv2.resize(img, IMAGE_SIZE)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "                img = img_to_array(img) / 255.0  # Normalize\n",
        "\n",
        "                images.append(img)\n",
        "                labels.append(class_idx)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {str(e)}\")\n",
        "\n",
        "            # Early stopping if we have enough samples\n",
        "            if num_samples and len(images) >= num_samples:\n",
        "                break\n",
        "\n",
        "        if num_samples and len(images) >= num_samples:\n",
        "            break\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def main():\n",
        "    setup_directories()\n",
        "\n",
        "    try:\n",
        "        # Download dataset using kagglehub\n",
        "        print(f\"Downloading dataset {DATASET_NAME}...\")\n",
        "        dataset_path = kagglehub.dataset_download(DATASET_NAME)\n",
        "        print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "        # Find the main dataset folder (it might be nested)\n",
        "        dataset_subfolder = None\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            if \"House_Room_Dataset\" in dirs:\n",
        "                dataset_subfolder = os.path.join(root, \"House_Room_Dataset\")\n",
        "                break\n",
        "\n",
        "        if not dataset_subfolder:\n",
        "            raise FileNotFoundError(\"Could not find folder in downloaded files\")\n",
        "\n",
        "        print(\"\\nDataset contents:\", os.listdir(dataset_subfolder))\n",
        "\n",
        "        # Load and preprocess images\n",
        "        X_images, Y = load_images_from_folder(dataset_subfolder, NUM_SAMPLES)\n",
        "        print(f\"\\nSuccessfully loaded {len(X_images)} images\")\n",
        "        print(f\"Image shape: {X_images[0].shape}\")\n",
        "        print(f\"Labels shape: {Y.shape}\")\n",
        "\n",
        "        # Split dataset (stratified to maintain class balance)\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "            X_images, Y, test_size=0.2, random_state=42, stratify=Y\n",
        "        )\n",
        "\n",
        "        # Save processed data\n",
        "        np.save(os.path.join(PREPROCESSED_FOLDER, \"X_train.npy\"), X_train)\n",
        "        np.save(os.path.join(PREPROCESSED_FOLDER, \"X_test.npy\"), X_test)\n",
        "        np.save(os.path.join(PREPROCESSED_FOLDER, \"Y_train.npy\"), Y_train)\n",
        "        np.save(os.path.join(PREPROCESSED_FOLDER, \"Y_test.npy\"), Y_test)\n",
        "\n",
        "        print(f\"\\nPreprocessed data saved to '{PREPROCESSED_FOLDER}' folder\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        print(\"Ensure you have a proper kagglehub setup\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JEY_N2d06Z-",
        "outputId": "e6863648-8396-4dea-cc81-0a94e233452e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset robinreni/house-rooms-image-dataset...\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/robinreni/house-rooms-image-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 116M/116M [00:00<00:00, 151MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/robinreni/house-rooms-image-dataset/versions/1\n",
            "\n",
            "Dataset contents: ['Bathroom', 'Kitchen', 'Bedroom', 'Livingroom', 'Dinning']\n",
            "Found classes: ['Bathroom', 'Bedroom', 'Dinning', 'Kitchen', 'Livingroom']\n",
            "\n",
            "Loading images from: /root/.cache/kagglehub/datasets/robinreni/house-rooms-image-dataset/versions/1/House_Room_Dataset/Bathroom\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 606/606 [00:00<00:00, 1116.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading images from: /root/.cache/kagglehub/datasets/robinreni/house-rooms-image-dataset/versions/1/House_Room_Dataset/Bedroom\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1248/1248 [00:01<00:00, 1121.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading images from: /root/.cache/kagglehub/datasets/robinreni/house-rooms-image-dataset/versions/1/House_Room_Dataset/Dinning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1158/1158 [00:01<00:00, 1024.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading images from: /root/.cache/kagglehub/datasets/robinreni/house-rooms-image-dataset/versions/1/House_Room_Dataset/Kitchen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 965/965 [00:00<00:00, 974.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading images from: /root/.cache/kagglehub/datasets/robinreni/house-rooms-image-dataset/versions/1/House_Room_Dataset/Livingroom\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1272/1273 [00:01<00:00, 735.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully loaded 5250 images\n",
            "Image shape: (128, 128, 3)\n",
            "Labels shape: (5250,)\n",
            "\n",
            "Preprocessed data saved to 'preprocessed_data' folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mikhail Ma House Rooms & Streets** Kaggle dataset (No Price)\n",
        "\n",
        "- Open the link and follow the steps from the first code block: https://www.kaggle.com/datasets/mikhailma/house-rooms-streets-image-dataset\n",
        "\n",
        "- Set a limit with the max_images_per_class configuration based on your RAM usage and storage (this dataset has several images)"
      ],
      "metadata": {
        "id": "6WohoDqm9erW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"dataset_slug\": \"mikhailma/house-rooms-streets-image-dataset\",\n",
        "    \"image_size\": (128, 128),\n",
        "    \"preprocessed_folder\": \"preprocessed_data\",\n",
        "    \"max_images_per_class\": 2000, # Set a limit here\n",
        "    \"test_size\": 0.2,\n",
        "    \"random_state\": 42\n",
        "}\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Create necessary directories and verify dependencies\"\"\"\n",
        "    os.makedirs(CONFIG[\"preprocessed_folder\"], exist_ok=True)\n",
        "\n",
        "    # Check if OpenCV is installed\n",
        "    if cv2.__version__ is None:\n",
        "        raise ImportError(\"Issue with OpenCV\")\n",
        "\n",
        "def load_and_preprocess_images(dataset_path):\n",
        "    \"\"\"Load images with preprocessing and class balancing\"\"\"\n",
        "    # Find main folder\n",
        "    dataset_subfolder = None\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        if \"kaggle_room_street_data\" in dirs:\n",
        "            dataset_subfolder = os.path.join(root, \"kaggle_room_street_data\")\n",
        "            break\n",
        "\n",
        "    if not dataset_subfolder:\n",
        "        raise FileNotFoundError(\"Could not find 'kaggle_room_street_data' folder\")\n",
        "\n",
        "    print(\"\\nDataset contents:\", os.listdir(dataset_subfolder))\n",
        "\n",
        "    class_names = sorted([d for d in os.listdir(dataset_subfolder)\n",
        "                       if os.path.isdir(os.path.join(dataset_subfolder, d))])\n",
        "\n",
        "    if not class_names:\n",
        "        raise ValueError(\"No class folders found in dataset\")\n",
        "\n",
        "    print(\"Found classes:\", class_names)\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_counts = {class_name: 0 for class_name in class_names}\n",
        "\n",
        "    for class_idx, class_name in enumerate(class_names):\n",
        "        class_folder = os.path.join(dataset_subfolder, class_name)\n",
        "        image_files = [f for f in os.listdir(class_folder)\n",
        "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        print(f\"\\nProcessing {len(image_files)} images from {class_name}\")\n",
        "\n",
        "        # Apply limit per class if specified\n",
        "        if CONFIG[\"max_images_per_class\"]:\n",
        "            image_files = image_files[:CONFIG[\"max_images_per_class\"]]\n",
        "            print(f\"Limiting to {CONFIG['max_images_per_class']} images per class\")\n",
        "\n",
        "        for filename in tqdm(image_files):\n",
        "            img_path = os.path.join(class_folder, filename)\n",
        "\n",
        "            try:\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is None:\n",
        "                    print(f\"Warning: Failed to load {img_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Preprocessing pipeline\n",
        "                img = cv2.resize(img, CONFIG[\"image_size\"])\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "                img = img_to_array(img) / 255.0  # Normalize\n",
        "\n",
        "                images.append(img)\n",
        "                labels.append(class_idx)\n",
        "                class_counts[class_name] += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {str(e)}\")\n",
        "\n",
        "    print(\"\\nFinal class distribution:\")\n",
        "    for class_name, count in class_counts.items():\n",
        "        print(f\"{class_name}: {count} images\")\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def save_data(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Save processed data with validation\"\"\"\n",
        "    try:\n",
        "        np.save(os.path.join(CONFIG[\"preprocessed_folder\"], \"X_train.npy\"), X_train)\n",
        "        np.save(os.path.join(CONFIG[\"preprocessed_folder\"], \"X_test.npy\"), X_test)\n",
        "        np.save(os.path.join(CONFIG[\"preprocessed_folder\"], \"y_train.npy\"), y_train)\n",
        "        np.save(os.path.join(CONFIG[\"preprocessed_folder\"], \"y_test.npy\"), y_test)\n",
        "\n",
        "        # Verify saved files\n",
        "        for f in [\"X_train.npy\", \"X_test.npy\", \"y_train.npy\", \"y_test.npy\"]:\n",
        "            if not os.path.exists(os.path.join(CONFIG[\"preprocessed_folder\"], f)):\n",
        "                raise FileNotFoundError(f\"Failed to save {f}\")\n",
        "\n",
        "        print(\"\\nData successfully saved to:\", CONFIG[\"preprocessed_folder\"])\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving data: {str(e)}\")\n",
        "\n",
        "def main():\n",
        "    setup_environment()\n",
        "\n",
        "    try:\n",
        "        # Download dataset\n",
        "        print(f\"Downloading dataset {CONFIG['dataset_slug']}...\")\n",
        "        dataset_path = kagglehub.dataset_download(CONFIG[\"dataset_slug\"])\n",
        "        print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "        X, y = load_and_preprocess_images(dataset_path)\n",
        "\n",
        "        # Stratified split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y,\n",
        "            test_size=CONFIG[\"test_size\"],\n",
        "            random_state=CONFIG[\"random_state\"],\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        print(\"\\nDataset split:\")\n",
        "        print(f\"Train: {X_train.shape[0]} samples\")\n",
        "        print(f\"Test: {X_test.shape[0]} samples\")\n",
        "\n",
        "        save_data(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        print(\"Ensure you have a proper kagglehub setup\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "K77K683xBohU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277ba1a2-6613-4043-d90a-c7d4f875f66e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset mikhailma/house-rooms-streets-image-dataset...\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mikhailma/house-rooms-streets-image-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 295M/295M [00:01<00:00, 164MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/mikhailma/house-rooms-streets-image-dataset/versions/1\n",
            "\n",
            "Dataset contents: ['house_data', 'street_data']\n",
            "Found classes: ['house_data', 'street_data']\n",
            "\n",
            "Processing 5249 images from house_data\n",
            "Limiting to 2000 images per class\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:02<00:00, 956.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing 19658 images from street_data\n",
            "Limiting to 2000 images per class\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:01<00:00, 1052.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final class distribution:\n",
            "house_data: 2000 images\n",
            "street_data: 2000 images\n",
            "\n",
            "Dataset split:\n",
            "Train: 3200 samples\n",
            "Test: 800 samples\n",
            "\n",
            "Data successfully saved to: preprocessed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GitHub** dataset with Price\n",
        "\n",
        "Dataset containing 2140 images, 4 images for each house. Also contains a text file that contains the textual metadata of the dataset. More information here: https://github.com/emanhamed/Houses-dataset/blob/master/README.md\n",
        "\n",
        "- First commented code portion indicates a fix in the case of running with a preexisting dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "Gz1SMqm29WIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tqdm import tqdm\n",
        "\n",
        "# First remove existing dataset if needed\n",
        "if os.path.exists(\"/content/Houses-dataset\"):\n",
        "    !rm -rf /content/Houses-dataset\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/emanhamed/Houses-dataset.git /content/Houses-dataset\n",
        "\n",
        "# Set paths\n",
        "dataset_path = \"/content/Houses-dataset/Houses Dataset\"\n",
        "info_file = os.path.join(dataset_path, \"HousesInfo.txt\")\n",
        "output_folder = \"/content/processed_houses\"\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"image_size\": (224, 224),\n",
        "    \"max_houses\": None,\n",
        "    \"image_types\": [\"bedroom\", \"bathroom\", \"kitchen\", \"frontal\"],\n",
        "    \"normalize_prices\": True\n",
        "}\n",
        "\n",
        "def load_metadata(info_path):\n",
        "    \"\"\"Load and process the HousesInfo.txt file\"\"\"\n",
        "    column_names = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "    metadata = pd.read_csv(info_path, sep=\" \", header=None, names=column_names,\n",
        "                         converters={\"bathrooms\": lambda x: float(x)})\n",
        "\n",
        "    metadata[\"house_id\"] = range(1, len(metadata)+1)\n",
        "    metadata[\"zipcode\"] = metadata[\"zipcode\"].astype(int).astype(str)\n",
        "\n",
        "    if CONFIG[\"normalize_prices\"]:\n",
        "        price_mean = metadata[\"price\"].mean()\n",
        "        price_std = metadata[\"price\"].std()\n",
        "        metadata[\"price_normalized\"] = (metadata[\"price\"] - price_mean) / price_std\n",
        "\n",
        "    return metadata\n",
        "\n",
        "def load_and_process_images(metadata):\n",
        "    \"\"\"Load and preprocess images\"\"\"\n",
        "    images = {}\n",
        "    house_data = {}\n",
        "\n",
        "    for house_id in metadata[\"house_id\"]:\n",
        "        images[house_id] = {img_type: None for img_type in CONFIG[\"image_types\"]}\n",
        "        house_data[house_id] = metadata[metadata[\"house_id\"] == house_id].iloc[0].to_dict()\n",
        "\n",
        "    image_files = [f for f in os.listdir(dataset_path)\n",
        "                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    for filename in tqdm(image_files, desc=\"Processing images\"):\n",
        "        try:\n",
        "            parts = filename.split(\"_\")\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "\n",
        "            house_id = int(parts[0])\n",
        "            img_type = parts[1].split(\".\")[0].lower()\n",
        "\n",
        "            if img_type not in CONFIG[\"image_types\"] or house_id not in images:\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(dataset_path, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, CONFIG[\"image_size\"])\n",
        "            images[house_id][img_type] = img_to_array(img) / 255.0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return images, house_data\n",
        "\n",
        "def save_processed_data(images, house_data):\n",
        "    \"\"\"Save processed data\"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Save per-house data\n",
        "    complete_houses = []\n",
        "    for house_id, img_data in images.items():\n",
        "        if all(img is not None for img in img_data.values()):\n",
        "            complete_houses.append({\n",
        "                \"house_id\": house_id,\n",
        "                **house_data[house_id],\n",
        "                **{f\"image_{k}\": v for k, v in img_data.items()}\n",
        "            })\n",
        "\n",
        "    # Save numpy arrays\n",
        "    house_images = np.array([[\n",
        "        h[f\"image_{img_type}\"] for img_type in CONFIG[\"image_types\"]\n",
        "    ] for h in complete_houses])\n",
        "\n",
        "    np.save(os.path.join(output_folder, \"house_images.npy\"), house_images)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata_df = pd.DataFrame(complete_houses)\n",
        "    metadata_df.to_csv(os.path.join(output_folder, \"metadata.csv\"), index=False)\n",
        "\n",
        "    print(f\"\\nSaved {len(complete_houses)} complete houses\")\n",
        "    print(f\"Images shape: {house_images.shape}\")\n",
        "    print(f\"Metadata columns: {metadata_df.columns.tolist()}\")\n",
        "\n",
        "# Main processing\n",
        "try:\n",
        "    print(\"Starting dataset processing...\")\n",
        "\n",
        "    metadata = load_metadata(info_file)\n",
        "    if CONFIG[\"max_houses\"]:\n",
        "        metadata = metadata.head(CONFIG[\"max_houses\"])\n",
        "\n",
        "    images, house_data = load_and_process_images(metadata)\n",
        "    save_processed_data(images, house_data)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6AMj5APwdEf",
        "outputId": "15184ea3-7bc7-4e2c-a5f3-20deacd0ff62",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/Houses-dataset'...\n",
            "remote: Enumerating objects: 2166, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 2166 (delta 0), reused 0 (delta 0), pack-reused 2165 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2166/2166), 176.26 MiB | 37.79 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n",
            "Updating files: 100% (2144/2144), done.\n",
            "Starting dataset processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 2140/2140 [00:10<00:00, 210.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved 535 complete houses\n",
            "Images shape: (535, 4, 224, 224, 3)\n",
            "Metadata columns: ['house_id', 'bedrooms', 'bathrooms', 'area', 'zipcode', 'price', 'price_normalized', 'image_bedroom', 'image_bathroom', 'image_kitchen', 'image_frontal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unified data loader**"
      ],
      "metadata": {
        "id": "0ougPvqtIOdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class HousePriceDataLoader:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.datasets = {\n",
        "            'ted8080': self._load_ted8080,\n",
        "            'github': self._load_github,\n",
        "            'robinreni': self._load_robinreni,\n",
        "            'mikhailma': self._load_mikhailma\n",
        "        }\n",
        "\n",
        "    def load_dataset(self, dataset_name, data_path):\n",
        "        \"\"\"Main loader interface\"\"\"\n",
        "        if dataset_name not in self.datasets:\n",
        "            raise ValueError(f\"Unknown dataset: {dataset_name}. Choose from {list(self.datasets.keys())}\")\n",
        "        return self.datasets[dataset_name](data_path)\n",
        "\n",
        "    def _load_ted8080(self, path):\n",
        "        \"\"\"Load preprocessed Ted8080 data\"\"\"\n",
        "        X = np.load(os.path.join(path, \"X_processed.npy\"))\n",
        "        y = np.load(os.path.join(path, \"y.npy\"))\n",
        "\n",
        "        # Ted8080 specific processing\n",
        "        X = self.scaler.fit_transform(X)\n",
        "        return X, y, None  # No images in this dataset\n",
        "\n",
        "    def _load_github(self, path):\n",
        "        \"\"\"Load GitHub houses dataset\"\"\"\n",
        "        images = np.load(os.path.join(path, \"house_images.npy\"))\n",
        "        metadata = pd.read_csv(os.path.join(path, \"metadata.csv\"))\n",
        "\n",
        "        # Extract features\n",
        "        X_tabular = metadata[['bedrooms', 'bathrooms', 'area']].values\n",
        "        X_tabular = self.scaler.fit_transform(X_tabular)\n",
        "        y = metadata['price'].values\n",
        "\n",
        "        return X_tabular, y, images\n",
        "\n",
        "    def _load_robinreni(self, path):\n",
        "        \"\"\"Load Robinreni dataset (classification)\"\"\"\n",
        "        X_train = np.load(os.path.join(path, \"X_train.npy\"))\n",
        "        X_test = np.load(os.path.join(path, \"X_test.npy\"))\n",
        "        y_train = np.load(os.path.join(path, \"Y_train.npy\"))\n",
        "        y_test = np.load(os.path.join(path, \"Y_test.npy\"))\n",
        "\n",
        "        # Combine train/test\n",
        "        X = np.concatenate([X_train, X_test])\n",
        "        y = np.concatenate([y_train, y_test])\n",
        "\n",
        "        return None, y, X  # Using images only\n",
        "\n",
        "    def _load_mikhailma(self, path):\n",
        "        \"\"\"Load Mikhailma dataset\"\"\"\n",
        "        X_train = np.load(os.path.join(path, \"X_train.npy\"))\n",
        "        y_train = np.load(os.path.join(path, \"y_train.npy\"))\n",
        "\n",
        "        return None, y_train, X_train  # Using images only"
      ],
      "metadata": {
        "id": "KMBztjBBIM90"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline Model**"
      ],
      "metadata": {
        "id": "WbYtT6ItK70T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "class HousePriceBaseline:\n",
        "    def __init__(self):\n",
        "        self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "    def train(self, X_tabular, X_images, y):\n",
        "        \"\"\"Train on combined features\"\"\"\n",
        "        # Extract image features if available\n",
        "        if X_images is not None:\n",
        "            img_features = np.array([img.mean(axis=(1,2)) for img in X_images])  # Simple channel means\n",
        "            X = np.concatenate([X_tabular, img_features], axis=1) if X_tabular is not None else img_features\n",
        "        else:\n",
        "            X = X_tabular\n",
        "\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_tabular, X_images):\n",
        "        if X_images is not None:\n",
        "            img_features = np.array([img.mean(axis=(1,2)) for img in X_images])\n",
        "            X = np.concatenate([X_tabular, img_features], axis=1) if X_tabular is not None else img_features\n",
        "        else:\n",
        "            X = X_tabular\n",
        "\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def evaluate(self, X_tabular, X_images, y_true):\n",
        "        y_pred = self.predict(X_tabular, X_images)\n",
        "        return {\n",
        "            'mae': mean_absolute_error(y_true, y_pred),\n",
        "            'r2': self.model.score(X_tabular if X_images is None else\n",
        "                                 np.concatenate([X_tabular, img_features], axis=1), y_true)\n",
        "        }"
      ],
      "metadata": {
        "id": "dsuqtKjOJBJx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning Model (CNN)"
      ],
      "metadata": {
        "id": "aNVa1qZaLI7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class HousePriceCNN:\n",
        "    def __init__(self, img_shape=(224, 224, 3), num_tabular_features=3):\n",
        "        self.img_shape = img_shape\n",
        "        self.num_tabular_features = num_tabular_features\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Image branch\n",
        "        img_input = Input(shape=self.img_shape)\n",
        "        x = Conv2D(32, (3,3), activation='relu')(img_input)\n",
        "        x = tf.keras.layers.MaxPooling2D()(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        # Tabular branch\n",
        "        if self.num_tabular_features > 0:\n",
        "            tab_input = Input(shape=(self.num_tabular_features,))\n",
        "            y = Dense(16, activation='relu')(tab_input)\n",
        "            combined = Concatenate()([x, y])\n",
        "        else:\n",
        "            tab_input = None\n",
        "            combined = x\n",
        "\n",
        "        # Regression head\n",
        "        z = Dense(64, activation='relu')(combined)\n",
        "        output = Dense(1)(z)\n",
        "\n",
        "        # Create model\n",
        "        inputs = [img_input, tab_input] if tab_input is not None else img_input\n",
        "        return Model(inputs=inputs, outputs=output)\n",
        "\n",
        "    def compile(self, lr=0.001):\n",
        "        self.model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(lr),\n",
        "            loss='mae',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "    def train(self, X_images, X_tabular, y, epochs=10, batch_size=32, val_split=0.2):\n",
        "        inputs = [X_images, X_tabular] if X_tabular is not None else X_images\n",
        "        history = self.model.fit(\n",
        "            inputs, y,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=val_split\n",
        "        )\n",
        "        return history"
      ],
      "metadata": {
        "id": "e61x4NvVLBSK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = HousePriceDataLoader()\n",
        "X_tabular, y, X_images = data_loader.load_dataset('github', '/content/processed_houses')\n",
        "\n",
        "# Reshape X_images to combine the 4 image types into a single channel\n",
        "X_images = X_images.reshape(-1, 224, 224, 12)  # 4 images * 3 channels = 12 channels\n",
        "\n",
        "# Initialize and train the CNN model with the updated input shape\n",
        "cnn_model = HousePriceCNN(img_shape=(224, 224, 12), num_tabular_features=3) # Adjust image shape\n",
        "cnn_model.compile(lr=0.0001) # Smaller learning rate\n",
        "cnn_model.train(X_images, X_tabular, y, epochs=15, batch_size=16) # Adjust parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTPbiHXiEMfE",
        "outputId": "46bbed54-c99c-413b-db37-c5d781c263a7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1s/step - loss: 585052.0625 - mae: 585052.0625 - val_loss: 531567.3750 - val_mae: 531567.3750\n",
            "Epoch 2/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 568673.0000 - mae: 568673.0000 - val_loss: 528863.6875 - val_mae: 528863.6875\n",
            "Epoch 3/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 604653.8750 - mae: 604653.8750 - val_loss: 522587.8750 - val_mae: 522587.8750\n",
            "Epoch 4/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 585753.5000 - mae: 585753.5000 - val_loss: 511172.9375 - val_mae: 511172.9375\n",
            "Epoch 5/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - loss: 545566.5000 - mae: 545566.5000 - val_loss: 493736.0312 - val_mae: 493736.0312\n",
            "Epoch 6/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 539347.0625 - mae: 539347.0625 - val_loss: 470015.1875 - val_mae: 470015.1875\n",
            "Epoch 7/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 504233.0625 - mae: 504233.0625 - val_loss: 440207.0625 - val_mae: 440207.0625\n",
            "Epoch 8/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 505065.6562 - mae: 505065.6562 - val_loss: 407396.2188 - val_mae: 407396.2188\n",
            "Epoch 9/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - loss: 461307.0625 - mae: 461307.0625 - val_loss: 372361.5000 - val_mae: 372361.5000\n",
            "Epoch 10/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 488434.9062 - mae: 488434.9062 - val_loss: 337072.5625 - val_mae: 337072.5625\n",
            "Epoch 11/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 414128.2188 - mae: 414128.2188 - val_loss: 304000.2188 - val_mae: 304000.2188\n",
            "Epoch 12/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 439035.9062 - mae: 439035.9062 - val_loss: 270447.0625 - val_mae: 270447.0625\n",
            "Epoch 13/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - loss: 388909.4062 - mae: 388909.4062 - val_loss: 241005.4062 - val_mae: 241005.4062\n",
            "Epoch 14/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 359060.5000 - mae: 359060.5000 - val_loss: 216379.8750 - val_mae: 216379.8750\n",
            "Epoch 15/15\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - loss: 366928.5938 - mae: 366928.5938 - val_loss: 197668.5938 - val_mae: 197668.5938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7afb21c7e690>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the CNN model on the test set\n",
        "y_pred_cnn = cnn_model.model.predict([X_test, X_test])\n",
        "mae_cnn = mean_absolute_error(y_test, y_pred_cnn)\n",
        "print(f\"CNN MAE on test set: {mae_cnn}\")\n",
        "\n",
        "# Evaluate the baseline model on the test set\n",
        "baseline_model = HousePriceBaseline()\n",
        "baseline_model.train(X_tabular, X_images, y)\n",
        "y_pred_baseline = baseline_model.predict(X_test_tabular, X_test_images)\n",
        "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
        "print(f\"Baseline MAE on test set: {mae_baseline}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "jn-zbwIo8BIi",
        "outputId": "b8a0d5e8-1c28-4d6c-8525-7c7daf9e24af"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_test_images' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-383f8f218bf6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate the CNN model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_tabular\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmae_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CNN MAE on test set: {mae_cnn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "Zc-p_ztuG2CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJhGLUu-HYpR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}